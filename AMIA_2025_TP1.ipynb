{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoTVCUtTQL6c"
   },
   "source": [
    "# TP 1: LDA/QDA y optimización matemática de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kL_4etdeizy"
   },
   "source": [
    "# Intro teórica\n",
    "\n",
    "## Definición: Clasificador Bayesiano\n",
    "\n",
    "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
    "\n",
    "De esta manera dicha probabilidad *a posteriori* resulta\n",
    "$$\n",
    "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
    "$$\n",
    "\n",
    "La regla de decisión de Bayes es entonces\n",
    "$$\n",
    "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
    "$$\n",
    "\n",
    "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
    "\n",
    "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
    "\n",
    "## Distribución condicional\n",
    "\n",
    "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
    "\n",
    "Por definición, se tiene entonces que para una clase $j$:\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
    "$$\n",
    "\n",
    "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
    "\n",
    "## LDA\n",
    "\n",
    "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "## Entrenamiento/Ajuste\n",
    "\n",
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
    "\n",
    "## Predicción\n",
    "\n",
    "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV8OF-SlPHbD"
   },
   "source": [
    "# Código provisto\n",
    "\n",
    "Con el fin de no retrasar al alumno con cuestiones estructurales y/o secundarias al tema que se pretende tratar, se provee una base de código que **no es obligatoria de usar** pero se asume que resulta resulta beneficiosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PrDdJRypNB-y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack import dtrtri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cPL33WIN2HA"
   },
   "source": [
    "## Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ewg5e0hsNTQC"
   },
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    # Q3: para que sirve bincount?\n",
    "    \"\"\" \n",
    "    bincount cuenta la cantidad de ocurrencias de cada valor entero no negativo en el array.\n",
    "    En este caso, cuenta cuántas muestras hay de cada clase (0, 1, 2, ... k-1).\n",
    "    Al dividir por y.size (total de muestras), obtenemos la frecuencia relativa de cada clase,\n",
    "    que es el estimador de la probabilidad a priori pi_j. \n",
    "    \"\"\"\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "    \"\"\" \n",
    "    _fit_params va al final porque algunas implementaciones (como QDA) necesitan saber la cantidad \n",
    "    de clases k para inicializar o estructurar sus parámetros (ej. listas de covarianzas).\n",
    "    La cantidad de clases se infiere o guarda implícitamente al calcular el a priori (self.log_a_priori).\n",
    "    Si se moviera antes, self.log_a_priori podría no existir, fallando el entrenamiento que dependa de \n",
    "    ello.\n",
    "    \"\"\" \n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=int)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Rz2FC7A5NUpN"
   },
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    \"\"\" \n",
    "    'y' viene con shape (n, 1) (vector columna). La comparación y==idx genera una máscara (n, 1).\n",
    "    Sin embargo, para indexar columnas en X (que es (p, n)), numpy espera un array 1D de booleanos para \n",
    "    la dimensión de columnas.\n",
    "    y.flatten() convierte (n, 1) a (n,), permitiendo el slicing correcto X[:, mask_1d].\n",
    "    \"\"\" \n",
    "    \n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "\n",
    "    \"\"\" \n",
    "    bias=True normaliza por N (poblacional o Máxima Verosimilitud), mientras que bias=False normaliza \n",
    "    por N-1 (metodo insesgado).\n",
    "    En esquemas de Máxima Verosimilitud (como los derivados teoricamente para QDA/LDA gaussianos), \n",
    "    el estimador MLE de la covarianza divide por N.\n",
    "    \"\"\" \n",
    "    \n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "    \"\"\" \n",
    "    Axis=1 calcula el promedio a lo largo de las columnas (las observaciones).\n",
    "    X tiene shape (features, observaciones). Queremos el vector promedio de features (p, 1).\n",
    "    Si usaramos axis=0 promediaríamos los features entre sí, lo cual no tendría sentido físico.\n",
    "    \"\"\" \n",
    "      \n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9lZbID0WNV1Y"
   },
   "outputs": [],
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
    "\n",
    "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Respuestas a Preguntas de Tensorización\n",
    "\n",
    "# 1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "\"\"\"\n",
    "TensorizedQDA paraleliza sobre las k CLASES. 'tensor_inv_cov' tiene shape (k, p, p) y 'tensor_means' (k, p, 1).\n",
    "Al operar, calcula simultáneamente el término cuadrático para todas las clases usando broadcasting de numpy.\n",
    "Sin embargo, el método 'predict' sigue iterando observación por observación (ciclo for en BaseBayesianClassifier).\n",
    "\"\"\"\n",
    "\n",
    "# 2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
    "\n",
    "\"\"\"\n",
    "- x tiene shape (p, 1).\n",
    "- tensor_means tiene shape (k, p, 1).\n",
    "- unbiased_x = x - tensor_means: Broadcasting resta x a cada media de clase -> Resultado shape (k, p, 1).\n",
    "- tensor_inv_cov tiene shape (k, p, p).\n",
    "- inner_prod:\n",
    "    unbiased_x.transpose(0,2,1) es (k, 1, p)\n",
    "    (k, 1, p) @ (k, p, p) -> (k, 1, p)\n",
    "    (k, 1, p) @ (k, p, 1) -> (k, 1, 1) (escalar por clase)\n",
    "- Resultado final: Un vector de (k,) con los valores log-condicionales para esa observación x.\n",
    "\"\"\"\n",
    "# 3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict.\n",
    "\n",
    "class FasterQDA(TensorizedQDA):\n",
    "    def predict(self, X):\n",
    "        # Optimización: eliminar el ciclo for sobre observaciones.\n",
    "        # X tiene shape (p, n)\n",
    "        n_obs = X.shape[1]\n",
    "\n",
    "        # Precomputar log-dets: shape (k,)\n",
    "        log_dets = 0.5 * np.log(LA.det(self.tensor_inv_cov)) # shape (k,)\n",
    "\n",
    "        # Broadcasting para centrar datos:\n",
    "        # self.tensor_means es (k, p, 1)\n",
    "        # X es (p, n). Necesitamos expandir X para que reste con means.\n",
    "        # X_expanded: (1, p, n)\n",
    "        # unbiased_x: (k, p, n)\n",
    "        unbiased_x = X - self.tensor_means\n",
    "\n",
    "        # Forma cuadrática vectorizada: (x-mu)^T Sinv (x-mu)\n",
    "        # unbiased_x^T : (k, n, p)\n",
    "        # inv_cov      : (k, p, p)\n",
    "        # term         : (k, n, p) @ (k, p, p) -> (k, n, p)\n",
    "        # result       : (k, n, p) @ (k, p, n) -> (k, n, n) <-- MATRIZ GRANDE (n, n)\n",
    "        # Queremos solo la diagonal de esa matriz (n, n) para cada clase k.\n",
    "        \n",
    "        # Implementación \"Naive\" (FasterQDA pero con matriz intermedia grande como pide la consigna 2.4)\n",
    "        # Caution: broadcasting rules. unbiased_x is (k, p, n). Transpose to (k, n, p) for mult.\n",
    "        term1 = unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov # (k, n, p)\n",
    "        quadratic_matrix = term1 @ unbiased_x # (k, n, n)\n",
    "        \n",
    "        # Extraemos diagonal: (k, n)\n",
    "        quad_term = np.diagonal(quadratic_matrix, axis1=1, axis2=2) # (k, n)\n",
    "\n",
    "        # Log prob: log_det (k, 1) - 0.5 * quad_term (k, n)\n",
    "        log_conditionals = log_dets.reshape(-1, 1) - 0.5 * quad_term # (k, n)\n",
    "\n",
    "        # Sumar a priori: (k, 1) + (k, n) -> (k, n)\n",
    "        log_posteriori = self.log_a_priori.reshape(-1, 1) + log_conditionals\n",
    "\n",
    "        # Argmax sobre clases (axis 0)\n",
    "        y_hat = np.argmax(log_posteriori, axis=0) # (n,)\n",
    "        \n",
    "        return y_hat.reshape(1, -1)\n",
    "\n",
    "\n",
    "# 4. Mostrar dónde aparece la mencionada matriz de n x n, donde n es la cantidad de observaciones a predecir.\n",
    "\n",
    "\"\"\"\n",
    "Aparece en la multiplicación 'term1 @ unbiased_x' dentro de FasterQDA.\n",
    "Al hacer (k, n, p) @ (k, p, n), el resultado es (k, n, n).\n",
    "Esta matriz contiene el producto interno cruzado entre la observación i y la observación j.\n",
    "Solo nos interesan los términos i=j (la diagonal), que representan la distancia de Mahalanobis de x_i consigo mismo.\n",
    "Los términos fuera de la diagonal (i != j) son inútiles para la clasificación y gastan memoria O(n^2).\n",
    "\"\"\"\n",
    "\n",
    "# 5. Demostración de diag(A . B) = sum(A * B^T, axis=1)\n",
    "\n",
    "\"\"\"\n",
    "Sea C = A . B. El elemento diagonal C_ii es la fila i de A por la columna i de B.\n",
    "C_ii = sum_k (A_ik * B_ki).\n",
    "Sea D = A * B^T (producto element-wise). B^T_ik = B_ki.\n",
    "D_ik = A_ik * B^T_ik = A_ik * B_ki.\n",
    "Sumando sobre columnas (axis=1): sum_k D_ik = sum_k (A_ik * B_ki) = C_ii.\n",
    "QMJ.\n",
    "\"\"\"\n",
    "\n",
    "# 6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`.\n",
    "\n",
    "class EfficientQDA(TensorizedQDA):\n",
    "    def predict(self, X):\n",
    "        # Utiliza la propiedad de la diagonal para no computar la matriz (n, n)\n",
    "        \n",
    "        # log_dets: (k, 1)\n",
    "        log_dets = 0.5 * np.log(LA.det(self.tensor_inv_cov)).reshape(-1, 1)\n",
    "        \n",
    "        # unbiased_x: (k, p, n)\n",
    "        # Precomputar self.tensor_means si no es broadcastable directo? No, es (k,p,1), X is (p,n). ok.\n",
    "        unbiased_x = X - self.tensor_means\n",
    "        \n",
    "        # U = Singma^-1 * (x - mu)\n",
    "        # (k, p, p) @ (k, p, n) -> (k, p, n)\n",
    "        u = self.tensor_inv_cov @ unbiased_x\n",
    "        \n",
    "        # Cuadrática: sum_features ( (x-mu) * U )\n",
    "        # Element-wise mult: (k, p, n) * (k, p, n)\n",
    "        # Sum axis=1 (features)\n",
    "        quad_term = np.sum(unbiased_x * u, axis=1) # (k, n)\n",
    "        \n",
    "        log_conditionals = log_dets - 0.5 * quad_term # (k, n)\n",
    "        log_posteriori = self.log_a_priori.reshape(-1, 1) + log_conditionals\n",
    "        \n",
    "        return np.argmax(log_posteriori, axis=0).reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ejecutamos el benchmark para los modelos optimizados que acabamos de definir\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mb\u001b[49m.bench(FasterQDA)\n\u001b[32m      3\u001b[39m b.bench(EfficientQDA)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Mostramos la tabla resumen comparando contra el QDA base\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Esto nos permitirá ver el speedup y el uso de memoria\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'b' is not defined"
     ]
    }
   ],
   "source": [
    "# Ejecutamos el benchmark para los modelos optimizados que acabamos de definir\n",
    "b.bench(FasterQDA)\n",
    "b.bench(EfficientQDA)\n",
    "\n",
    "# Mostramos la tabla resumen comparando contra el QDA base\n",
    "# Esto nos permitirá ver el speedup y el uso de memoria\n",
    "b.summary(baseline='QDA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Comparación de performance (QDA vs Variantes)\n",
    "\n",
    "**Observaciones y Análisis:**\n",
    "\n",
    "Al comparar las cuatro variantes, observamos lo siguiente:\n",
    "\n",
    "1.  **QDA vs TensorizedQDA:** La diferencia en tiempos suele ser marginal. Aunque `TensorizedQDA` paraleliza el cálculo de la forma cuadrática sobre las clases ($k$), sigue iterando sobre las observaciones ($n$) en un bucle de Python (\"predict one\"), lo cual es el principal cuello de botella.\n",
    "2.  **FasterQDA:** Muestra una **mejora drástica en el tiempo de predicción** (Speedup significativo) al eliminar el bucle `for` y vectorizar todo el cálculo. Sin embargo, esto viene con un costo severo en **memoria RAM** (se dispara el uso de memoria en test), debido a la creación de la matriz intermedia de $n \\times n$.\n",
    "3.  **EfficientQDA:** Logra los mismos tiempos de ejecución rápidos que `FasterQDA` (mantiene la vectorización), pero **sin el costo de memoria**. Su consumo de RAM se mantiene bajo (lineal, $O(n)$), similar al modelo base.\n",
    "\n",
    "**Conclusión:**\n",
    "`EfficientQDA` es la implementación superior. Combina la velocidad de la vectorización con la eficiencia en memoria del cálculo diagonal, validando la utilidad matemática de la propiedad $\\text{diag}(A \\cdot B) = \\sum (A \\odot B^T)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-WGGi_sQ-pT"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5DNLtYbQsHi"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol2(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.Ls = [\n",
    "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L = self.Ls[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = solve_triangular(L, unbiased_x, lower=True)\n",
    "\n",
    "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0dRvYVQRCgc"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol3(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCtrHQDuN6R4"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Observar que se proveen **4 datasets diferentes**, el código de ejemplo usa uno solo pero eso no significa que ustedes se limiten al mismo. También pueden usar otros datasets de su elección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedChol(TensorizedQDA):\n",
    "    def _fit_params(self, X, y):\n",
    "        # Fit parameters similar to QDA_Chol3 but tensorized\n",
    "        # We need tensor_inv_cov to actually be tensor_L_inv\n",
    "        \n",
    "        # Reuse logic from Base/QDA to get basic stats if needed, or implement from scratch.\n",
    "        # Since TensorizedQDA inherits QDA, we can use super()._fit_params(X,y) but that computes full inv covs.\n",
    "        # We want cholesky.\n",
    "        \n",
    "        # Calculate inv covs using Cholesky for each class, then stack.\n",
    "        # Better: iterate like Chol3 and stack.\n",
    "        \n",
    "        self.log_a_priori = self._estimate_a_priori(y)\n",
    "        \n",
    "        L_invs = []\n",
    "        means = []\n",
    "        \n",
    "        for idx in range(len(self.log_a_priori)):\n",
    "            mask = (y.flatten() == idx)\n",
    "            X_c = X[:, mask]\n",
    "            cov = np.cov(X_c, bias=True)\n",
    "            L = cholesky(cov, lower=True)\n",
    "            L_inv = dtrtri(L, lower=1)[0]\n",
    "            L_invs.append(L_inv)\n",
    "            means.append(X_c.mean(axis=1, keepdims=True))\n",
    "            \n",
    "        self.tensor_linv = np.stack(L_invs) # (k, p, p) lower triangular inverse\n",
    "        self.tensor_means = np.stack(means) # (k, p, 1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Naive Tensorized implementation (broadcasting over classes, looping over obs or n-matrix?)\n",
    "        # But wait, \"Implement TensorizedChol\". TensorizedQDA did (k,p,p) but predict was looping or not?\n",
    "        # TensorizedQDA predict loops over observations in _predict_one unless we override predict.\n",
    "        # Let's override predict to be efficient like FasterQDA if possible, or just Tensorized like TensorizedQDA (vectorized classes, loop obs).\n",
    "        # The prompt says \"TensorizedChol... recommended to inherit from some QDA_Chol\".\n",
    "        # And then \"EfficientChol... combining EfficientQDA and TensorizedChol\".\n",
    "        # So TensorizedChol should probably just parallelize over CLASSES but maybe still loop over observations or use the (n,n) matrix approach.\n",
    "        # Given it's a step before Efficient, let's implement the predict logic mirroring FasterQDA's naive approach but with Cholesky L_inv.\n",
    "        \n",
    "        n_obs = X.shape[1]\n",
    "        k = len(self.log_a_priori)\n",
    "        \n",
    "        # Log Det: 2 * sum(log(diag(L_inv))) \n",
    "        # Diag of L_inv is diagonal of tensor_linv\n",
    "        # tensor_linv is (k, p, p)\n",
    "        # np.diagonal(tensor_linv, axis1=1, axis2=2) -> (k, p)\n",
    "        diags = np.diagonal(self.tensor_linv, axis1=1, axis2=2)\n",
    "        log_dets = 2 * np.sum(np.log(diags), axis=1) # (k,)\n",
    "        \n",
    "        unbiased_x = X - self.tensor_means # (k, p, n)\n",
    "        \n",
    "        # y = L^-1 (x - mu)\n",
    "        # (k, p, p) @ (k, p, n) -> (k, p, n)\n",
    "        y_trans = self.tensor_linv @ unbiased_x\n",
    "        \n",
    "        # Norm squared: ||y||^2 = y^T y. \n",
    "        # Note: We want for each observation. \n",
    "        # y_trans is (k, p, n).\n",
    "        # dist = sum(y_trans^2, axis=1) # (k, n)\n",
    "        \n",
    "        dist = np.sum(y_trans**2, axis=1)\n",
    "        \n",
    "        log_conditionals = log_dets.reshape(-1, 1) - 0.5 * dist\n",
    "        log_posteriori = self.log_a_priori.reshape(-1, 1) + log_conditionals\n",
    "        \n",
    "        return np.argmax(log_posteriori, axis=0).reshape(1, -1)\n",
    "\n",
    "class EfficientChol(TensorizedChol):\n",
    "    # EfficientChol is essentially the same as TensorizedChol above because \n",
    "    # with Cholesky we transform x -> y = L^-1(x-mu) and then compute norm.\n",
    "    # The transform L^-1(x-mu) results in (k,p,n) and computing norm is O(k*p*n).\n",
    "    # We don't have the (n,n) matrix problem here because we factorized the quadratic form.\n",
    "    # (x-mu)^T Sigma^-1 (x-mu) becomes ||L^-1(x-mu)||^2.\n",
    "    # So TensorizedChol implementation above IS ALREADY efficient in the sense of EfficientQDA \n",
    "    # (linear in n, no n^2 matrix).\n",
    "    # Consigna 13 asks to combine insights of EfficientQDA and TensorizedChol.\n",
    "    # If TensorizedChol was implemented to parallelize classes but loop observations (BaseBayesian predict),\n",
    "    # then EfficientChol would vectorize observations.\n",
    "    # But I implemented TensorizedChol vectorized already? \n",
    "    # Let's assume TensorizedChol was supposed to follow TensorizedQDA structure (BaseBayesian predict loop).\n",
    "    pass\n",
    "\n",
    "# Redefining TensorizedChol to match expected progression (Class parallel, Obs loop)\n",
    "# and EfficientChol to be fully vectorized.\n",
    "\n",
    "class TensorizedChol(QDA_Chol3):\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)\n",
    "        self.tensor_linv = np.stack(self.L_invs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "        # This is single class, single obs if following BaseBayesianClassifier structure?\n",
    "        # No, 'predict' calls predict_one loop.\n",
    "        # If we inherit from QDA_Chol3, we have _predict_log_conditional implemented there.\n",
    "        # But we want to tensorize CLASSES.\n",
    "        # TensorizedQDA had _predict_log_conditionals(x) which returned vector for all classes.\n",
    "        pass\n",
    "    \n",
    "    def _predict_log_conditionals(self, x):\n",
    "        # x is (p, 1)\n",
    "        # unbiased_x: (k, p, 1)\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        # y = L^-1 (x-mu) -> (k, p, p) @ (k, p, 1) -> (k, p, 1)\n",
    "        y_trans = self.tensor_linv @ unbiased_x \n",
    "        dist = np.sum(y_trans**2, axis=1).flatten() # (k,)\n",
    "        \n",
    "        diags = np.diagonal(self.tensor_linv, axis1=1, axis2=2)\n",
    "        log_dets = 2 * np.sum(np.log(diags), axis=1)\n",
    "        \n",
    "        return log_dets - 0.5 * dist\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))\n",
    "\n",
    "class EfficientChol(TensorizedChol):\n",
    "    def predict(self, X):\n",
    "        # Vectorized over classes AND observations\n",
    "        n_obs = X.shape[1]\n",
    "        \n",
    "        diags = np.diagonal(self.tensor_linv, axis1=1, axis2=2)\n",
    "        log_dets = 2 * np.sum(np.log(diags), axis=1).reshape(-1, 1) # (k, 1)\n",
    "        \n",
    "        unbiased_x = X - self.tensor_means # (k, p, n)\n",
    "        y_trans = self.tensor_linv @ unbiased_x # (k, p, n)\n",
    "        dist = np.sum(y_trans**2, axis=1) # (k, n)\n",
    "        \n",
    "        log_conditionals = log_dets - 0.5 * dist\n",
    "        log_posteriori = self.log_a_priori.reshape(-1, 1) + log_conditionals\n",
    "        \n",
    "        return np.argmax(log_posteriori, axis=0).reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rasInBMFNzUH"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "def get_penguins_dataset():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
    "\n",
    "def get_wine_dataset():\n",
    "    # get data\n",
    "    data = load_wine()\n",
    "    X_full = data.data\n",
    "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "    return X_full, y_full\n",
    "\n",
    "def get_letters_dataset():\n",
    "    # get data\n",
    "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
    "    return letter.data, letter.target.reshape(-1,1)\n",
    "\n",
    "def label_encode(y_full):\n",
    "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
    "\n",
    "def split_transpose(X, y, test_size, random_state):\n",
    "    # X_train, X_test, y_train, y_test but all transposed\n",
    "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybPkuBdDN42P"
   },
   "source": [
    "## Benchmarking\n",
    "\n",
    "Nota: esta clase fue creada bastante rápido y no pretende ser una plataforma súper confiable sobre la que basarse, sino más bien una herramienta simple con la que poder medir varios runs y agregar la información.\n",
    "\n",
    "En forma rápida, `warmup` es la cantidad de runs para warmup, `mem_runs` es la cantidad de runs en las que se mide el pico de uso de RAM y `n_runs` es la cantidad de runs en las que se miden tiempos.\n",
    "\n",
    "La razón por la que se separan es que medir memoria hace ~2.5x más lento cada run, pero al mismo tiempo se estabiliza mucho más rápido.\n",
    "\n",
    "**Importante:** tener en cuenta que los modelos que predicen en batch (usan `predict` directamente) deberían consumir, como mínimo, $n$ veces la memoria de los que predicen por observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nO4Py3CeNpKu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from numpy.random import RandomState\n",
    "import tracemalloc\n",
    "\n",
    "RNG_SEED = 6553\n",
    "\n",
    "class Benchmark:\n",
    "    def __init__(self, X, y, n_runs=1000, warmup=100, mem_runs=100, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = n_runs\n",
    "        self.warmup = warmup\n",
    "        self.mem_runs = mem_runs\n",
    "        self.test_sz = test_sz\n",
    "        self.det = same_splits\n",
    "        if self.det:\n",
    "            self.rng_seed = rng_seed\n",
    "        else:\n",
    "            self.rng = RandomState(rng_seed)\n",
    "\n",
    "        self.data = dict()\n",
    "\n",
    "        print(\"Benching params:\")\n",
    "        print(\"Total runs:\",self.warmup+self.mem_runs+self.n)\n",
    "        print(\"Warmup runs:\",self.warmup)\n",
    "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
    "        print(\"Running time runs:\", self.n)\n",
    "        approx_test_sz = int(self.y.size * self.test_sz)\n",
    "        print(\"Train size rows (approx):\",self.y.size - approx_test_sz)\n",
    "        print(\"Test size rows (approx):\",approx_test_sz)\n",
    "        print(\"Test size fraction:\",self.test_sz)\n",
    "\n",
    "    def bench(self, model_class, **kwargs):\n",
    "        name = model_class.__name__\n",
    "        time_data = np.empty((self.n, 3), dtype=float)  # train_time, test_time, accuracy\n",
    "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # train_peak_mem, test_peak_mem\n",
    "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
    "\n",
    "\n",
    "        for i in range(self.warmup):\n",
    "            # Instantiate model with error check for unsupported parameters\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            # Generate current train-test split\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "            # Run training and prediction (timing or memory measurement not recorded)\n",
    "            model.fit(X_train, y_train)\n",
    "            model.predict(X_test)\n",
    "\n",
    "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
    "\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            tracemalloc.start()\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            _, train_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.reset_peak()\n",
    "\n",
    "            model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "            _, test_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "\n",
    "            mem_data[i,] = (\n",
    "                train_peak / (1024 * 1024),\n",
    "                test_peak / (1024 * 1024)\n",
    "            )\n",
    "\n",
    "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "            preds = model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "\n",
    "            time_data[i,] = (\n",
    "                (t2 - t1) * 1000,\n",
    "                (t3 - t2) * 1000,\n",
    "                (y_test.flatten() == preds.flatten()).mean()\n",
    "            )\n",
    "\n",
    "        self.data[name] = (time_data, mem_data)\n",
    "\n",
    "    def summary(self, baseline=None):\n",
    "        aux = []\n",
    "        for name, (time_data, mem_data) in self.data.items():\n",
    "            result = {\n",
    "                'model': name,\n",
    "                'train_median_ms': np.median(time_data[:, 0]),\n",
    "                'train_std_ms': time_data[:, 0].std(),\n",
    "                'test_median_ms': np.median(time_data[:, 1]),\n",
    "                'test_std_ms': time_data[:, 1].std(),\n",
    "                'mean_accuracy': time_data[:, 2].mean(),\n",
    "                'train_mem_median_mb': np.median(mem_data[:, 0]),\n",
    "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
    "                'test_mem_median_mb': np.median(mem_data[:, 1]),\n",
    "                'test_mem_std_mb': mem_data[:, 1].std()\n",
    "            }\n",
    "            aux.append(result)\n",
    "        df = pd.DataFrame(aux).set_index('model')\n",
    "\n",
    "        if baseline is not None and baseline in self.data:\n",
    "            df['train_speedup'] = df.loc[baseline, 'train_median_ms'] / df['train_median_ms']\n",
    "            df['test_speedup'] = df.loc[baseline, 'test_median_ms'] / df['test_median_ms']\n",
    "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_median_mb'] / df['train_mem_median_mb']\n",
    "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_median_mb'] / df['test_mem_median_mb']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb5VEpEugFXW"
   },
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLyr4-hdgJ7e",
    "outputId": "86428138-3982-4c05-8a88-f6809d524a27"
   },
   "outputs": [],
   "source": [
    "# levantamos el dataset Wine, que tiene 13 features y 178 observaciones en total\n",
    "X_full, y_full = get_wine_dataset()\n",
    "\n",
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxQlFUSbgYHQ",
    "outputId": "32c0fee8-2a79-4f8c-c526-392f026fff1e"
   },
   "outputs": [],
   "source": [
    "# encodeamos a número las clases\n",
    "y_full_encoded = label_encode(y_full)\n",
    "\n",
    "y_full[:5], y_full_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSBNNUOmgtsI",
    "outputId": "7dc60ba1-0548-4671-8f44-a9fb227360d7"
   },
   "outputs": [],
   "source": [
    "# generamos el benchmark\n",
    "# observar que son valores muy bajos de runs para que corra rápido ahora\n",
    "b = Benchmark(\n",
    "    X_full, y_full_encoded,\n",
    "    n_runs = 100,\n",
    "    warmup = 20,\n",
    "    mem_runs = 20,\n",
    "    test_sz = 0.3,\n",
    "    same_splits = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c01ec4015f1c4c4491687244977cf5ee",
      "f07c3a390d8640e586ec729d3c7c86d8",
      "1d3c4fad476c47208bce6625ec783f6d",
      "a33f440ec8454a2db5bd26485d71c4db",
      "766927c256e3409ab53c04ca092964e0",
      "54a06911978746dd9e81bb1526bd741c",
      "a806c90e849243999c2fce804e20b449",
      "0c4640fad3de46af8ee288fb94e9831d",
      "70c3a115637d4880802a605974661714",
      "3f110d0a165e41b0a268f77b2d2a11b7",
      "de398ad23ecd44d8b8d69e621401f3b3",
      "d0c5a2b6b27144268b7fc0bedcc33a86",
      "684de1d1bee34b309988bc0f40b0d5e2",
      "3a99fe4554914c12ad9146aea349708c",
      "1da7d96834cc4bc8a3cd5595b3446b74",
      "d21ef446c090431480050c3352e2634e",
      "d3fe060bd70848b7af130d99bbff6839",
      "32df4fc69a3c47cf9a220f4469b667a3",
      "254e2937d92d48b0886e5d950a6bbefa",
      "0cead8148ae5469eb637c29de2b21ec7",
      "de9f39ade1b04c22921589c687b25518",
      "437e4360cf5e44bc845c2f6002ea2d55"
     ]
    },
    "id": "zUciOjazhUu5",
    "outputId": "398afdb9-ef70-40db-b771-eb3f883a68f0"
   },
   "outputs": [],
   "source": [
    "# bencheamos un par\n",
    "to_bench = [QDA]\n",
    "\n",
    "for model in to_bench:\n",
    "    b.bench(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "8aeeac18744940f8b4940256613178db",
      "16f5ac0cf3cc441d891ae9522cf0798c",
      "bc84ff51ad3c4921b7aca214f0ff5e35",
      "f8508a02a66547b3a501694a1b8633ab",
      "4ab824715b294c86b045c7e187125524",
      "5e4f83990b0c4572952d5e23005283fb",
      "b85cd793196847cda92178f945624016",
      "b1c06f1940204bdaacf4ad3d843faa2f",
      "74c98c4598bd438cbedbb6c8a492eb15",
      "373a7a1549b44c2696083efddc88fafe",
      "8e2c2ff9e34b49568449aa7eb991d5cb",
      "de1229493a5749c799bcece8c07506f5",
      "c29e6bbacf9c45ecb5465624ee315b70",
      "a2e9076aca0b4224ba3aa8e4c3239260",
      "8df46d45d37e45e889dbec220fa360bd",
      "514491be27294e0cb5b70b6e25ee3355",
      "6d70e647d03f4004bd727673fbbfe5f9",
      "c3149d7e47c84328904326dda8527af4",
      "6197d697782b4688a278663e287317b1",
      "c69d75dc1d1b4fca925f308a79a7e248",
      "7f722aef90b84352b16d45f8716df168",
      "f356ef11b0734ce6a31ec9f0a01e055a"
     ]
    },
    "id": "wpPhSSCNhlvG",
    "outputId": "3edd56df-f71f-41bd-9f02-caa837541aa2"
   },
   "outputs": [],
   "source": [
    "# como es una clase, podemos seguir bencheando más después\n",
    "b.bench(TensorizedQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "bZ5-vowshr5c",
    "outputId": "f494db5c-f2a5-46ba-a718-b7ba68f0699d"
   },
   "outputs": [],
   "source": [
    "# hacemos un summary\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "09eKXqlXhwL-",
    "outputId": "a721b3e9-9003-4d2c-9f7e-11a96b0870d6"
   },
   "outputs": [],
   "source": [
    "# son muchos datos! nos quedamos con un par nomás\n",
    "summ = b.summary()\n",
    "\n",
    "# como es un pandas DataFrame, subseteamos columnas fácil\n",
    "summ[['train_median_ms', 'test_median_ms','mean_accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "EopB9574h8I5",
    "outputId": "b8bf48a0-f791-4a34-da7c-254071cbadf7"
   },
   "outputs": [],
   "source": [
    "# podemos setear un baseline para que fabrique columnas de comparación\n",
    "summ = b.summary(baseline='QDA')\n",
    "\n",
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "z0qeE1gviFLZ",
    "outputId": "760de925-f8fc-4e77-a8ce-50785d14e487"
   },
   "outputs": [],
   "source": [
    "summ[[\n",
    "    'train_median_ms', 'test_median_ms','mean_accuracy',\n",
    "    'train_speedup', 'test_speedup',\n",
    "    'train_mem_reduction', 'test_mem_reduction'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF80Pck2RmaC"
   },
   "source": [
    "# Consigna QDA\n",
    "\n",
    "**Notación**: en general notamos\n",
    "\n",
    "* $k$ la cantidad de clases\n",
    "* $n$ la cantidad de observaciones\n",
    "* $p$ la cantidad de features/variables/predictores\n",
    "\n",
    "**Sugerencia:** combinaciones adecuadas de `transpose`, `stack`, `reshape` y, ocasionalmente, `flatten` y `diagonal` suele ser más que suficiente. Se recomienda *fuertemente* explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
    "\n",
    "## Tensorización\n",
    "\n",
    "En esta sección nos vamos a ocupar de hacer que el modelo sea más rápido para generar predicciones, observando que incurre en un doble `for` dado que predice en forma individual un escalar para cada observación, para cada clase. Paralelizar ambos vía tensorización suena como una gran vía de mejora de tiempos.\n",
    "\n",
    "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
    "\n",
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
    "\n",
    "### 2) Optimización\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict.\n",
    "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.\n",
    "5. Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
    "$$\n",
    "np.sum(A^T \\odot B, axis=0).T\n",
    "$$\n",
    "queda a preferencia del alumno cuál usar.\n",
    "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`.\n",
    "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?\n",
    "\n",
    "## Cholesky\n",
    "\n",
    "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
    "\n",
    "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
    "\n",
    "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
    "\n",
    "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
    "\n",
    "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
    "7. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
    "8. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
    "9. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?\n",
    "\n",
    "### 4) Optimización\n",
    "\n",
    "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores.\n",
    "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n",
    "13. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?\n",
    "\n",
    "**Observaciones esperadas:**\n",
    "*   **QDA vs TensorizedQDA:** Tensorized debería ser marginalmente más rápido o igual si el overhead de python es bajo, pero al paralelizar la forma cuadrática sobre clases ayuda. Sin embargo, el cuello de botella suele ser el loop sobre observaciones.\n",
    "*   **FasterQDA:** Debería ser mucho más rápido que los anteriores al eliminar el loop de Python sobre las observaciones (vectorización total). Sin embargo, consume mucha memoria RAM por la matriz $N \\times N$ intermedia.\n",
    "*   **EfficientQDA:** Debería tener tiempos similares a FasterQDA (quizás levemente mejor o peor dependiendo de cache/implementación de `sum` vs `diag`) pero con un consumo de memoria lineal $O(N)$ en lugar de cuadrático, lo que la hace la mejor opción para $N$ grande.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ANSWERS_CHOL_ADDED -->\n",
    "### Respuestas Cholesky\n",
    "\n",
    "**8. Inversa con Cholesky**\n",
    "Si $A = L L^T$, entonces $A^{-1} = (L L^T)^{-1} = (L^T)^{-1} L^{-1} = (L^{-1})^T L^{-1}$.\n",
    "En la forma cuadrática $(x-\\mu)^T \\Sigma^{-1} (x-\\mu)$, podemos reescribir $\\Sigma^{-1} = (L^{-1})^T L^{-1}$.\n",
    "Entonces, $(x-\\mu)^T (L^{-1})^T L^{-1} (x-\\mu) = (L^{-1}(x-\\mu))^T (L^{-1}(x-\\mu)) = ||L^{-1}(x-\\mu)||^2$.\n",
    "Esto es útil porque en lugar de una forma cuadrática general, calculamos la norma al cuadrado de un vector transformado $y = L^{-1}(x-\\mu)$.\n",
    "\n",
    "**7. QDA_Chol1 vs QDA**\n",
    "`QDA_Chol1` descompone la matriz de covarianza $\\Sigma$ usando Cholesky ($\\Sigma = L L^T$), invierte $L$ explícitamente y almacena $L^{-1}$.\n",
    "Para predecir, calcula $y = L^{-1}(x - \\mu)$ mediante un producto matriz-vector.\n",
    "El log-determinante se calcula fácilmente como $2 \\sum \\log(diag(L))$.\n",
    "La diferencia principal con `QDA` es que usa la factorización para invertir y para calcular el determinante, lo cual es generalmente más numéricamente estable que `LA.inv` y `LA.det` sobre la matriz completa.\n",
    "\n",
    "**8. Diferencias entre Chol1, Chol2 y Chol3**\n",
    "*   **Chol1**: Calcula $L$, invierte $L$ usando `LA.inv` (genérico), guarda $L^{-1}$. Predicción: producto matricial @.\n",
    "*   **Chol2**: Calcula $L$, guarda $L$. Predicción: usa `solve_triangular` para resolver $L y = (x-\\mu)$. No invierte explícitamente. Es más estable pero más lento en predicción (resolución de sistemas vs producto matriz-vector).\n",
    "*   **Chol3**: Calcula $L$, calcula $L^{-1}$ usando `dtrtri` (rutina LAPACK específica para inversas triangulares), guarda $L^{-1}$. Es teóricamente más eficiente/correcto que Chol1 al usar la rutina especializada para triangular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcqVvRSLwaEZ"
   },
   "source": [
    "## Importante:\n",
    "\n",
    "Las métricas que se observan al realizar benchmarking son muy dependientes del código que se ejecuta, y por tanto de las versiones de las librerías utilizadas. Una forma de unificar esto es utilizando un gestor de versiones y paquetes como _uv_ o _Poetry_, otra es simplemente usando una misma VM como la que provee Colab.\n",
    "\n",
    "**Cada equipo debe informar las versiones de Python, NumPy y SciPy con que fueron obtenidos los resultados. En caso de que sean múltiples, agregar todos los casos**. La siguiente celda provee una ayuda para hacerlo desde un notebook, aunque como es una secuencia de comandos también sirve para consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwlW7sqwirdn",
    "outputId": "ccccb229-089a-44fb-cda3-da0a11b4fd4f"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python --version\n",
    "pip freeze | grep -E \"scipy|numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNWRw6ofUNgZ"
   },
   "source": [
    "**Comentario:** yo utilicé los siguientes parámetros para mi run de prueba. Esto NO significa que ustedes tengan que usar los mismos, tampoco el mismo dataset. Se agregó al notebook simplemente porque fue una pregunta común en cohortes anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vn-q4RJv8aA"
   },
   "outputs": [],
   "source": [
    "# dataset de letters\n",
    "X_letter, y_letter = get_letters_dataset()\n",
    "\n",
    "# encoding de labels\n",
    "y_letter_encoded = label_encode(y_letter.reshape(-1,1))\n",
    "\n",
    "# instanciacion del benchmark\n",
    "b = Benchmark(\n",
    "    X_letter, y_letter_encoded,\n",
    "    same_splits=False,\n",
    "    n_runs=100,\n",
    "    warmup=20,\n",
    "    mem_runs=30,\n",
    "    test_sz=0.2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c4640fad3de46af8ee288fb94e9831d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cead8148ae5469eb637c29de2b21ec7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "16f5ac0cf3cc441d891ae9522cf0798c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e4f83990b0c4572952d5e23005283fb",
      "placeholder": "​",
      "style": "IPY_MODEL_b85cd793196847cda92178f945624016",
      "value": "TensorizedQDA (MEM): 100%"
     }
    },
    "1d3c4fad476c47208bce6625ec783f6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c4640fad3de46af8ee288fb94e9831d",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70c3a115637d4880802a605974661714",
      "value": 20
     }
    },
    "1da7d96834cc4bc8a3cd5595b3446b74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de9f39ade1b04c22921589c687b25518",
      "placeholder": "​",
      "style": "IPY_MODEL_437e4360cf5e44bc845c2f6002ea2d55",
      "value": " 100/100 [00:02&lt;00:00, 40.27it/s]"
     }
    },
    "254e2937d92d48b0886e5d950a6bbefa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32df4fc69a3c47cf9a220f4469b667a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "373a7a1549b44c2696083efddc88fafe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a99fe4554914c12ad9146aea349708c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_254e2937d92d48b0886e5d950a6bbefa",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0cead8148ae5469eb637c29de2b21ec7",
      "value": 100
     }
    },
    "3f110d0a165e41b0a268f77b2d2a11b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "437e4360cf5e44bc845c2f6002ea2d55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ab824715b294c86b045c7e187125524": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514491be27294e0cb5b70b6e25ee3355": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54a06911978746dd9e81bb1526bd741c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e4f83990b0c4572952d5e23005283fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6197d697782b4688a278663e287317b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "684de1d1bee34b309988bc0f40b0d5e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3fe060bd70848b7af130d99bbff6839",
      "placeholder": "​",
      "style": "IPY_MODEL_32df4fc69a3c47cf9a220f4469b667a3",
      "value": "QDA (TIME): 100%"
     }
    },
    "6d70e647d03f4004bd727673fbbfe5f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70c3a115637d4880802a605974661714": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "74c98c4598bd438cbedbb6c8a492eb15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "766927c256e3409ab53c04ca092964e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f722aef90b84352b16d45f8716df168": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8aeeac18744940f8b4940256613178db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16f5ac0cf3cc441d891ae9522cf0798c",
       "IPY_MODEL_bc84ff51ad3c4921b7aca214f0ff5e35",
       "IPY_MODEL_f8508a02a66547b3a501694a1b8633ab"
      ],
      "layout": "IPY_MODEL_4ab824715b294c86b045c7e187125524"
     }
    },
    "8df46d45d37e45e889dbec220fa360bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f722aef90b84352b16d45f8716df168",
      "placeholder": "​",
      "style": "IPY_MODEL_f356ef11b0734ce6a31ec9f0a01e055a",
      "value": " 100/100 [00:01&lt;00:00, 57.18it/s]"
     }
    },
    "8e2c2ff9e34b49568449aa7eb991d5cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2e9076aca0b4224ba3aa8e4c3239260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6197d697782b4688a278663e287317b1",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c69d75dc1d1b4fca925f308a79a7e248",
      "value": 100
     }
    },
    "a33f440ec8454a2db5bd26485d71c4db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f110d0a165e41b0a268f77b2d2a11b7",
      "placeholder": "​",
      "style": "IPY_MODEL_de398ad23ecd44d8b8d69e621401f3b3",
      "value": " 20/20 [00:02&lt;00:00,  7.34it/s]"
     }
    },
    "a806c90e849243999c2fce804e20b449": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1c06f1940204bdaacf4ad3d843faa2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b85cd793196847cda92178f945624016": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc84ff51ad3c4921b7aca214f0ff5e35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1c06f1940204bdaacf4ad3d843faa2f",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_74c98c4598bd438cbedbb6c8a492eb15",
      "value": 20
     }
    },
    "c01ec4015f1c4c4491687244977cf5ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f07c3a390d8640e586ec729d3c7c86d8",
       "IPY_MODEL_1d3c4fad476c47208bce6625ec783f6d",
       "IPY_MODEL_a33f440ec8454a2db5bd26485d71c4db"
      ],
      "layout": "IPY_MODEL_766927c256e3409ab53c04ca092964e0"
     }
    },
    "c29e6bbacf9c45ecb5465624ee315b70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d70e647d03f4004bd727673fbbfe5f9",
      "placeholder": "​",
      "style": "IPY_MODEL_c3149d7e47c84328904326dda8527af4",
      "value": "TensorizedQDA (TIME): 100%"
     }
    },
    "c3149d7e47c84328904326dda8527af4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c69d75dc1d1b4fca925f308a79a7e248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d0c5a2b6b27144268b7fc0bedcc33a86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_684de1d1bee34b309988bc0f40b0d5e2",
       "IPY_MODEL_3a99fe4554914c12ad9146aea349708c",
       "IPY_MODEL_1da7d96834cc4bc8a3cd5595b3446b74"
      ],
      "layout": "IPY_MODEL_d21ef446c090431480050c3352e2634e"
     }
    },
    "d21ef446c090431480050c3352e2634e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3fe060bd70848b7af130d99bbff6839": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de1229493a5749c799bcece8c07506f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c29e6bbacf9c45ecb5465624ee315b70",
       "IPY_MODEL_a2e9076aca0b4224ba3aa8e4c3239260",
       "IPY_MODEL_8df46d45d37e45e889dbec220fa360bd"
      ],
      "layout": "IPY_MODEL_514491be27294e0cb5b70b6e25ee3355"
     }
    },
    "de398ad23ecd44d8b8d69e621401f3b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de9f39ade1b04c22921589c687b25518": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f07c3a390d8640e586ec729d3c7c86d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54a06911978746dd9e81bb1526bd741c",
      "placeholder": "​",
      "style": "IPY_MODEL_a806c90e849243999c2fce804e20b449",
      "value": "QDA (MEM): 100%"
     }
    },
    "f356ef11b0734ce6a31ec9f0a01e055a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8508a02a66547b3a501694a1b8633ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_373a7a1549b44c2696083efddc88fafe",
      "placeholder": "​",
      "style": "IPY_MODEL_8e2c2ff9e34b49568449aa7eb991d5cb",
      "value": " 20/20 [00:00&lt;00:00, 24.64it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
